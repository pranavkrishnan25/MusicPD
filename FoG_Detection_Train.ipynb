{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input, Dense, Flatten,Conv1D, Bidirectional, LSTM, MaxPooling1D, BatchNormalization, Dropout, Layer\n",
    "from keras.models import Sequential\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import Precision, Recall\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28424, 256, 3) (28424, 1)\n",
      "(7107, 256, 3) (7107, 1)\n"
     ]
    }
   ],
   "source": [
    "X_data = np.load(\"Processed DaphNet/final_x.npy\")\n",
    "Y_data = np.load(\"Processed DaphNet/final_y.npy\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(Attention,self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "                               initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "                               initializer=\"zeros\")\n",
    "        \n",
    "        super(Attention,self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        \n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "def Specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "model= Sequential()\n",
    "\n",
    "model.add(Input(shape=[256, 3]))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Attention(return_sequences=False))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss=BinaryCrossentropy(), metrics=[\"binary_accuracy\", \"AUC\", Recall(), Specificity, Precision()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_3 (Conv1D)           (None, 254, 64)           640       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 254, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 127, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 125, 64)           12352     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 125, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 62, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 60, 128)           24704     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 60, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 30, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 30, 256)          263168    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " attention_1 (Attention)     (None, 256)               286       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 318,687\n",
      "Trainable params: 318,175\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "889/889 [==============================] - 141s 143ms/step - loss: 0.2676 - binary_accuracy: 0.8862 - auc: 0.8902 - recall: 0.3935 - Specificity: 0.9698 - precision: 0.6879 - val_loss: 0.2460 - val_binary_accuracy: 0.8888 - val_auc: 0.9200 - val_recall: 0.3392 - val_Specificity: 0.9866 - val_precision: 0.8180\n",
      "Epoch 2/14\n",
      "889/889 [==============================] - 121s 136ms/step - loss: 0.2194 - binary_accuracy: 0.9058 - auc: 0.9329 - recall: 0.5856 - Specificity: 0.9604 - precision: 0.7134 - val_loss: 0.2656 - val_binary_accuracy: 0.8756 - val_auc: 0.9407 - val_recall: 0.8779 - val_Specificity: 0.8760 - val_precision: 0.5558\n",
      "Epoch 3/14\n",
      "889/889 [==============================] - 125s 140ms/step - loss: 0.2075 - binary_accuracy: 0.9102 - auc: 0.9401 - recall: 0.6012 - Specificity: 0.9627 - precision: 0.7316 - val_loss: 0.3101 - val_binary_accuracy: 0.8423 - val_auc: 0.9443 - val_recall: 0.9115 - val_Specificity: 0.8289 - val_precision: 0.4880\n",
      "Epoch 4/14\n",
      "889/889 [==============================] - 132s 148ms/step - loss: 0.2039 - binary_accuracy: 0.9133 - auc: 0.9421 - recall: 0.6356 - Specificity: 0.9607 - precision: 0.7312 - val_loss: 0.1981 - val_binary_accuracy: 0.9177 - val_auc: 0.9545 - val_recall: 0.5433 - val_Specificity: 0.9844 - val_precision: 0.8599\n",
      "Epoch 5/14\n",
      "889/889 [==============================] - 124s 140ms/step - loss: 0.1965 - binary_accuracy: 0.9179 - auc: 0.9462 - recall: 0.6679 - Specificity: 0.9602 - precision: 0.7407 - val_loss: 0.1943 - val_binary_accuracy: 0.9222 - val_auc: 0.9498 - val_recall: 0.7213 - val_Specificity: 0.9582 - val_precision: 0.7529\n",
      "Epoch 6/14\n",
      "889/889 [==============================] - 141s 159ms/step - loss: 0.2012 - binary_accuracy: 0.9169 - auc: 0.9433 - recall: 0.6502 - Specificity: 0.9622 - precision: 0.7444 - val_loss: 0.1873 - val_binary_accuracy: 0.9256 - val_auc: 0.9571 - val_recall: 0.7931 - val_Specificity: 0.9493 - val_precision: 0.7349\n",
      "Epoch 7/14\n",
      "889/889 [==============================] - 129s 146ms/step - loss: 0.1924 - binary_accuracy: 0.9215 - auc: 0.9482 - recall: 0.6732 - Specificity: 0.9638 - precision: 0.7582 - val_loss: 0.1725 - val_binary_accuracy: 0.9333 - val_auc: 0.9622 - val_recall: 0.7195 - val_Specificity: 0.9715 - val_precision: 0.8169\n",
      "Epoch 8/14\n",
      "889/889 [==============================] - 119s 134ms/step - loss: 0.1924 - binary_accuracy: 0.9216 - auc: 0.9480 - recall: 0.6599 - Specificity: 0.9662 - precision: 0.7671 - val_loss: 0.2104 - val_binary_accuracy: 0.9153 - val_auc: 0.9494 - val_recall: 0.5396 - val_Specificity: 0.9822 - val_precision: 0.8428\n",
      "Epoch 9/14\n",
      "889/889 [==============================] - 121s 136ms/step - loss: 0.1860 - binary_accuracy: 0.9227 - auc: 0.9524 - recall: 0.6764 - Specificity: 0.9649 - precision: 0.7638 - val_loss: 0.1722 - val_binary_accuracy: 0.9280 - val_auc: 0.9615 - val_recall: 0.6915 - val_Specificity: 0.9701 - val_precision: 0.8039\n",
      "Epoch 10/14\n",
      "889/889 [==============================] - 119s 133ms/step - loss: 0.1884 - binary_accuracy: 0.9235 - auc: 0.9505 - recall: 0.6820 - Specificity: 0.9644 - precision: 0.7651 - val_loss: 0.1898 - val_binary_accuracy: 0.9277 - val_auc: 0.9589 - val_recall: 0.7530 - val_Specificity: 0.9588 - val_precision: 0.7644\n",
      "Epoch 11/14\n",
      "889/889 [==============================] - 121s 136ms/step - loss: 0.1898 - binary_accuracy: 0.9231 - auc: 0.9498 - recall: 0.6749 - Specificity: 0.9654 - precision: 0.7668 - val_loss: 0.1815 - val_binary_accuracy: 0.9323 - val_auc: 0.9586 - val_recall: 0.7810 - val_Specificity: 0.9593 - val_precision: 0.7731\n",
      "Epoch 12/14\n",
      "889/889 [==============================] - 123s 138ms/step - loss: 0.1848 - binary_accuracy: 0.9236 - auc: 0.9528 - recall: 0.6710 - Specificity: 0.9669 - precision: 0.7722 - val_loss: 0.1820 - val_binary_accuracy: 0.9242 - val_auc: 0.9590 - val_recall: 0.7735 - val_Specificity: 0.9511 - val_precision: 0.7371\n",
      "Epoch 13/14\n",
      "889/889 [==============================] - 122s 137ms/step - loss: 0.1849 - binary_accuracy: 0.9237 - auc: 0.9529 - recall: 0.6887 - Specificity: 0.9634 - precision: 0.7619 - val_loss: 0.1860 - val_binary_accuracy: 0.9253 - val_auc: 0.9581 - val_recall: 0.7325 - val_Specificity: 0.9598 - val_precision: 0.7631\n",
      "Epoch 14/14\n",
      "889/889 [==============================] - 122s 138ms/step - loss: 0.1927 - binary_accuracy: 0.9185 - auc: 0.9487 - recall: 0.6618 - Specificity: 0.9621 - precision: 0.7472 - val_loss: 0.1848 - val_binary_accuracy: 0.9225 - val_auc: 0.9597 - val_recall: 0.8164 - val_Specificity: 0.9414 - val_precision: 0.7122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15a2f89e3d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, batch_size=BATCH_SIZE, epochs=14, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.137048e-08]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[0].reshape(1, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\patel_223gv41\\AppData\\Local\\Temp\\tmp4zoun_gg\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\patel_223gv41\\AppData\\Local\\Temp\\tmp4zoun_gg\\assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/baseline_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223/223 [==============================] - 9s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.25189998745918274 with G-Mean: 0.894\n",
      "FPR: 0.1001, TPR: 0.8882\n"
     ]
    }
   ],
   "source": [
    "# Calculate the G-mean\n",
    "gmean = np.sqrt(tpr * (1 - fpr))\n",
    "\n",
    "# Find the optimal threshold\n",
    "index = np.argmax(gmean)\n",
    "thresholdOpt = round(thresholds[index], ndigits = 4)\n",
    "gmeanOpt = round(gmean[index], ndigits = 4)\n",
    "fprOpt = round(fpr[index], ndigits = 4)\n",
    "tprOpt = round(tpr[index], ndigits = 4)\n",
    "print('Best Threshold: {} with G-Mean: {}'.format(thresholdOpt, gmeanOpt))\n",
    "print('FPR: {}, TPR: {}'.format(fprOpt, tprOpt))\n",
    "print('Sensitivity: {}, Specificity: {}'.format(tprOpt, 1 - fprOpt))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebc54687aefdcd75e400cbdf1055601fb5433e4cbcb3dedb2c792bb13c2d3644"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
